# Public Records AI Assistant — Full Backlog (Markdown)
**Generated:** 2025-08-27  
**Scope:** Prototype-first Google/GCP stack (Next.js + MUI, Firestore/Cloud SQL (dummy), Vertex AI Matching Engine, DLP/DocAI, Gmail API mock sends), us-west residency.

---

## Legend
- **US-###** = User Story ID  
- **AC** = Acceptance Criteria (Gherkin-style)  
- **[ ]** = Task item (checklist)

---

# Epic 0 — Foundation & Environments
**Goal:** A working Google-first prototype stack with code quality gates and us-west residency.

### US-000: Bootstrap project
**AC**
- Given the repo is cloned, when I run `npm install && npm run dev`, then the app boots locally with MUI/Next.
- Given I push to `main`, when CI runs, then lint/type checks succeed and the build artifact is produced.
**Tasks**
- [ ] Create repo and Next.js + TypeScript + MUI baseline.
- [ ] Add ESLint + Prettier + TS strict config.
- [ ] Set up GitHub Actions/Cloud Build pipelines (lint/test/build).
- [ ] Add `.env.example` for Google Identity/keys (mocked for Phase 0).
- [ ] Add top-level README and contributing guide.

### US-001: us-west residency guardrails
**AC**
- Given any storage/database/service, region shows **us-west** (or nearest supported regional).
- Given logs export, destination dataset is in a us-west region.
**Tasks**
- [ ] Infra defaults (Terraform or ADR docs) with us-west regions.
- [ ] Region guardrails doc + static analysis/pre-commit check for IaC.

---

# Epic 1 — Request Intake (Public Portal)
**Goal:** Citizens can submit requests; system generates ID and queues it.

### US-010: Submit a public records request
**AC**
- Given I complete required fields, when I submit, then I receive a tracking ID and confirmation page.
- Given invalid input, then I see field-level errors and cannot submit.
**Tasks**
- [ ] MUI form (name, email, description, optional date range, attachments).
- [ ] Client validation (Zod) + a11y (WCAG 2.1 AA).
- [ ] Persist to Firestore (prototype) or dummy DB service.
- [ ] Save attachments to mock Storage location or `/public/records` (Phase 0).
- [ ] Confirmation page with tracking ID and printable summary.

### US-011: Intake queue for staff
**AC**
- Given new requests, when I open the console, then I see requests ordered by received date with status `Open`.
**Tasks**
- [ ] Requests grid (MUI Data Grid), sortable/filterable.
- [ ] Status chips (Open/In Progress/Under Review/Completed).
- [ ] Row click → request details drawer/page.

---

# Epic 2 — Agency Console & SLA
**Goal:** Staff work from queues with SLA and filters.

### US-020: Filter by agency/status/due date
**AC**
- Given filters are set, when I apply, then the grid updates and the URL reflects filters.
**Tasks**
- [ ] Filter bar with agency/status/date.
- [ ] URL param sync (shareable views).
- [ ] Empty-state and reset filters.

### US-021: SLA clocks & due-soon indicators
**AC**
- Given a request due in ≤3 business days, then it’s flagged “Due Soon”; if past due, “Overdue”.
**Tasks**
- [ ] Business-day utility for due date math.
- [ ] Conditional styling and column badges.

---

# Epic 3 — Search & AI Match (Vertex Matching Engine)
**Goal:** AI produces candidate records with confidence and rationale.

### US-030: Run AI match and view Top‑N
**AC**
- Given a request, when I run “Find Matches,” then I see ≥5 candidates with score and rationale.
- Given no good matches, then I see an empty state with helpful suggestions.
**Tasks**
- [ ] “Find Matches” action in request details.
- [ ] Phase 0: load from `matches.csv` (mock service); design API for Vertex Matching Engine.
- [ ] Explainability UI: key phrases + distance score.

### US-031: Accept/Reject candidate matches
**AC**
- Given a candidate, when I accept, then it’s marked “selected for review”; when rejected, it’s excluded.
**Tasks**
- [ ] Accept/Reject controls and local store updates.
- [ ] Persist selection state to dummy DB.
- [ ] Audit entry (“match_decision”).

---

# Epic 4 — Redaction (DLP + Human Review)
**Goal:** PII suggestions with manual overrides; 100% human review.

### US-040: View suggested PII findings
**AC**
- Given a record, when I open it, then I see a PDF preview plus structured PII findings (if any).
- Given no findings, then I see “No PII detected.”
**Tasks**
- [ ] PDF.js integration for preview.
- [ ] Findings panel hydrate from `redactions.csv` (Phase 0).
- [ ] Toggle overlays (Phase 0: stub rectangles).

### US-041: Draw redactions and export rendition
**AC**
- Given I draw boxes, when I save, then a new redaction version and coordinates are stored.
- Given I export, then I get a redacted PDF rendition.
**Tasks**
- [ ] Canvas overlay for mouse/touch boxes.
- [ ] Save “versioned redactions” (JSON) in dummy DB.
- [ ] Prototype export: placeholder PDF + metadata; prod hook to server pipeline.
- [ ] Audit entry (“redaction_saved”).

### US-042: Human approval gate (100% review)
**AC**
- Given a package has redactions, when not approved, delivery is blocked; once approved, it’s unblocked.
**Tasks**
- [ ] Approval gate on delivery action.
- [ ] Approval UI (approve/request changes).
- [ ] Audit entry (“redaction_approved”).

---

# Epic 5 — Approvals & Legal Review
**Goal:** Formal review with comments and status.

### US-050: Request changes
**AC**
- Given I request changes, the record status switches to `changes_requested` and owner is notified (mock).
**Tasks**
- [ ] Comment threads UI at record level.
- [ ] Status transitions.
- [ ] Audit entry (“changes_requested”).

### US-051: Approve package for release
**AC**
- Given all selected records, when I approve, the package is locked for delivery.
**Tasks**
- [ ] Package approval endpoint (mock) + lock flag.
- [ ] Visual indicator “Approved for Release”.
- [ ] Audit entry (“package_approved”).

---

# Epic 6 — Package & Delivery (Mock Sends)
**Goal:** Combine records into single PDF; schedule & mock send.

### US-060: Build combined package with cover sheet & index
**AC**
- Given selected records, when I click “Build Package,” I see a preview with page order & index.
**Tasks**
- [ ] Package builder UI (ordering, title page).
- [ ] Package manifest JSON + placeholder combined PDF.
- [ ] Audit entry (“package_built”).

### US-061: Schedule mock delivery
**AC**
- Given a package, when I schedule, a delivery row is recorded with method ∈ {{`mock_email`, `portal_link`}} and scheduled time.
- Given schedule time reached (simulated), status becomes `sent` and an audit entry exists.
**Tasks**
- [ ] Delivery form (method/time).
- [ ] Mock scheduler (client timer) + status transitions.
- [ ] Signed URL placeholder string for portal.
- [ ] Audit entry (“delivery_scheduled”/“delivery_sent”).

---

# Epic 7 — Audit & Observability (BigQuery)
**Goal:** Auditable actions, BigQuery export for dashboards.

### US-070: Immutable application audit log
**AC**
- Given an action occurs, then `audit` row is written with actor, action, subject, timestamp, and context.
**Tasks**
- [ ] Client event logger; persist to dummy audit table.
- [ ] Audit panel on request details with filtering.
- [ ] Privacy pass (PII not logged in clear).

### US-071: BigQuery export for dashboards
**AC**
- Given export is enabled, BigQuery dataset contains `events`, `deliveries`, `errors` tables.
**Tasks**
- [ ] Define BQ schema (events, deliveries, errors) and dataset name (e.g., `pr_ai_audit_us_west`).
- [ ] Mock exporter from CSV/JSON to BQ schema (Phase 0 docs or script).
- [ ] Looker Studio/SQL examples for key KPIs (turnaround, backlog, SLA breaches).

---

# Epic 8 — Synthetic Data & Public‑Domain Corpus
**Goal:** Multi-agency realistic data for demo.

### US-080: Load synthetic dataset v2 (multi-agency)
**AC**
- Given I drop CSVs into `/public/data`, the app loads agencies/requests/records/matches/redactions/deliveries.
**Tasks**
- [ ] Seed script docs; link dataset zip.
- [ ] “Reload data” dev utility.
- [ ] Deterministic IDs for demo script.

### US-081: Import public‑domain PDFs (PII‑free)
**AC**
- Given I add PDFs to a folder, they appear as candidates (metadata mocked).
**Tasks**
- [ ] Corpus import utility (filename → `records.csv` row).
- [ ] License/PII‑free checklist doc.
- [ ] Tag source = `PublicDomain` in metadata.

---

# Epic 9 — RBAC & Multi‑Agency
**Goal:** Multi-tenant views and permissions (prototype scope).

### US-090: Agency switcher & row filtering
**AC**
- Given agency A, when I switch to agency B, the grid shows only B’s data.
**Tasks**
- [ ] Agency switcher in top bar.
- [ ] Row-level filtering by `agency_id` (client-side for Phase 0).

### US-091: Role-based UI
**AC**
- Given a user role (Admin, Records Officer, Legal Reviewer), screens/features adhere to RBAC.
**Tasks**
- [ ] Mock JWT claims → role store.
- [ ] Guard actions (approve, deliver) and hide/disable controls.
- [ ] Role badges.

---

# Epic 10 — Non-functional & Readiness
**Goal:** Performance, accessibility, reliability checks.

### US-100: Accessibility & responsiveness
**AC**
- Keyboard navigation reaches all interactive elements with visible focus.
- Mobile layouts avoid horizontal scroll and maintain tap targets ≥ 44px.
**Tasks**
- [ ] Axe scan and manual a11y pass.
- [ ] Keyboard traps/focus management fixes.
- [ ] Responsive layout QA.

### US-101: Performance SLOs (demo data)
**AC**
- P95 latencies meet target budgets in common flows (table load, match view, viewer).
**Tasks**
- [ ] Lightweight timing hooks & console traces.
- [ ] k6 script for list loads (local).

---

# Epic 11 — “Go Demo” (Prototype)
**Goal:** A crisp end-to-end demo path.

### US-110: Scripted end‑to‑end demo
**AC**
- I can: create request → run matches → review/redact → approve → build package → schedule mock send → show audit trail.
**Tasks**
- [ ] Write demo script with exact IDs & paths.
- [ ] Seed deterministic dataset matching the script.
- [ ] “Reset demo data” utility (reload defaults).

---

# Cross‑Epic — AI Email Drafting (Prototype)
### US-120: Generate response draft
**AC**
- Given a request, when I click “Draft Response”, I receive a prefilled template (request ID, summary, delivery method) ready for edit.
**Tasks**
- [ ] Draft panel with legal-friendly template variables.
- [ ] Save draft to request notes; copy-to-clipboard.
- [ ] (Later) Connect to Gemini prompt with guardrails & disclaimer.

---

## Definition of Done (per story)
- Code merged with lint/tests passing.
- A11y checks for any UI.
- Audit entries written for user-triggered actions.
- Updated README/Docs where applicable.
- Demo path verified if story is part of E2E.

## Suggested Sprint Plan (example)
- **Sprint 1:** Epics 0–1 (foundation, intake, queues).  
- **Sprint 2:** Epics 3–4 (match prototype, redaction viewer).  
- **Sprint 3:** Epics 5–6 (approvals, package & delivery mock).  
- **Sprint 4:** Epics 7–8 (audit→BQ, corpora import, multi-agency filters).  
- **Sprint 5:** Epics 9–11 (RBAC polish, perf/a11y, go-demo), AI email draft.
# Product Requirements Document (PRD)

**Product Name:** Public Records AI Assistant (working title)\
**Owner:** \[State IT Agency / WaTech / Project Owner\]\
**Version:** Draft v1.0\
**Date:** \[Insert Date\]

------------------------------------------------------------------------

## 1. Overview

This product will provide a **responsive web application** that enables
state agencies to intake, manage, and fulfill public records requests.
The system leverages **AI models** to match requests with relevant
public records across multiple databases, streamline redaction, and
support compliance with public disclosure laws.

By centralizing intake, management, and delivery of public records, the
solution will reduce processing time, improve accuracy, and ensure
transparency while protecting sensitive information.

------------------------------------------------------------------------

## 2. Goals & Objectives

-   **Efficiency:** Reduce manual work in locating, reviewing, and
    preparing records.\
-   **Accuracy:** Use AI to improve record-to-request matching.\
-   **Compliance:** Support state/federal regulations for public records
    and PII protection.\
-   **Transparency:** Provide requesters with clear statuses and timely
    responses.\
-   **Scalability:** Support multiple state agencies and integration
    with diverse record systems.

------------------------------------------------------------------------

## 3. Key Features

### 3.1 User Management

-   Secure login (SSO, multi-factor authentication, role-based access).\
-   Roles: Administrator, Records Officer, Legal Reviewer, Agency Staff,
    Requester.\
-   Permissions: Define access for record retrieval, redaction,
    approvals, etc.

### 3.2 Public Records Request Intake

-   Web form for requesters (responsive, mobile-friendly).\
-   Automatic assignment of tracking ID.\
-   Ability to upload attachments (supporting evidence,
    clarifications).\
-   Request categorization and tagging (e.g., subject matter,
    timeframe).

### 3.3 Request Management Dashboard

-   Centralized queue for all incoming requests.\
-   Filtering (status, requester, due date, category).\
-   SLA tracking with alerts for approaching deadlines.\
-   Communication log for each request.

### 3.4 AI-Powered Record Matching

-   Connects to agency databases (structured and unstructured).\
-   Natural language AI model maps requests to relevant records.\
-   Confidence scoring for matches (High/Medium/Low).\
-   Preview of matched records before confirmation.

### 3.5 Redaction Flows

-   Web-based redaction editor for PDFs, images, text.\
-   Automatic AI-suggested PII detection (names, SSN, DOB, addresses,
    etc.).\
-   Manual review and override by records officers.\
-   Version history and audit trail.

### 3.6 Approvals & Legal Review

-   Workflow for routing redacted records to approvers/legal teams.\
-   Commenting and change requests within the app.\
-   Approval status logging (approved/rejected/changes requested).

### 3.7 Delivery & Scheduling

-   Ability to combine multiple records into a single PDF.\
-   Automated metadata and cover sheet generation.\
-   Option to schedule delivery (e.g., by deadline or after approval).\
-   Secure email with attachment or portal download link.

### 3.8 Tracking & Audit Trail

-   Full lifecycle tracking: Intake → Match → Redaction → Approval →
    Delivery.\
-   Request status indicators (Open, In Progress, Under Review,
    Completed).\
-   Audit log of every action taken on a record.

### 3.9 AI-Assisted Response Generation

-   AI model drafts professional, legally-compliant response emails.\
-   Suggests tone, format, and inserts metadata (request ID, date,
    summary).\
-   Human review before sending.

------------------------------------------------------------------------

## 4. Non-Functional Requirements

### 4.1 Security & Compliance

-   CJIS/FedRAMP compliant hosting.\
-   End-to-end encryption (in transit & at rest).\
-   Full role-based access control.\
-   PII masking and compliance with state public disclosure laws.

### 4.2 Performance

-   Must handle 10,000+ concurrent requests across multiple agencies.\
-   Sub-second AI record search on common queries.\
-   System availability SLA: 99.9%.

### 4.3 Integrations

-   API connectors for state databases (e.g., case management, HR, email
    archives).\
-   Integration with Microsoft 365 / Google Workspace for record
    ingestion.\
-   PDF management and digital signature integration (e.g., DocuSign,
    Adobe).

------------------------------------------------------------------------

## 5. User Flows

1.  **Requester Intake Flow:** Citizen submits a records request →
    system generates ID → assigned to agency staff.\
2.  **Records Officer Flow:** Logs in → views queue → system suggests
    matched records → officer confirms.\
3.  **Redaction Flow:** Records flagged → AI suggests redactions →
    officer reviews/edits → sends to legal.\
4.  **Approval Flow:** Legal reviews → approves or requests changes →
    upon approval, moves to delivery.\
5.  **Delivery Flow:** Records combined into PDF → AI drafts response →
    officer reviews → email sent → status updated.

------------------------------------------------------------------------

## 6. Success Metrics

-   **Request Turnaround Time** reduced by X%.\
-   **Accuracy of AI Record Matching** (target \>90% high-confidence
    match).\
-   **User Satisfaction** scores from agency staff & public requesters.\
-   **Reduction in Legal/Compliance Escalations**.\
-   **Adoption Rate Across Agencies**.

------------------------------------------------------------------------

## 7. Future Enhancements

-   Requester self-service portal for status checks.\
-   AI-powered translation for multi-language responses.\
-   Predictive analytics for request trends.\
-   Cross-agency record sharing and federated search.

# Design Document (DD) — Public Records AI Assistant
**Environment:** Google Cloud Platform (GCP)  
**Doc Owner:** [You]  
**Status:** Draft v1  
**Date:** 2025-08-27

---

## 1) Summary & Goals
This design specifies *how* we’ll implement the PRD on **GCP**, focusing on a Google‑first stack (auth, storage, email) and an interactive **prototype** that uses a **dummy dataset** and publicly available APIs to simulate full behavior. The app supports intake, AI‑assisted record matching, redaction with human review, approvals/legal, and scheduled delivery.

**Objectives**
- Reduce turnaround time with AI‑assisted retrieval and redaction.
- Ensure compliance with public‑records statutes and PII protection.
- Provide full auditability and multi‑agency scale.
- Prototype quickly using synthetic data and mock sends, then graduate to real connectors.

---

## 2) Architecture (GCP)
**High‑level (ASCII)**
```
[Public Portal (Next.js + MUI)] --Cloud CDN--> [Frontend on Cloud Run] --IAP--> [Backend APIs on Cloud Run]
                                                                     |-> Pub/Sub (async jobs)
                                                                     |-> Cloud Tasks (scheduled sends)
                                                                     |-> Firestore (prototype) / Cloud SQL (prod)
                                                                     |-> Cloud Storage (raw & renditions)
                                                                     |-> Vertex AI (Gemini, Embeddings)
                                                                     |-> Vertex Matching Engine (vector search)
                                                                     |-> Document AI (OCR)
                                                                     |-> DLP API (PII findings)
                                                                     |-> Secret Manager / KMS (CMEK)
                                                                     |-> Cloud Audit Logs -> BigQuery sink
External systems (future): SharePoint/Exchange, Google Drive/Gmail, file shares/SFTP, case mgmt DBs, on‑prem DBs, Azure, AWS
```

**Core choices**
- **Frontend**: React/Next.js (SSR where useful) with **TypeScript** and **MUI** for the component library & theming (design tokens, dark mode, RTL). Served via **Cloud Run** behind **Cloud CDN** + **Cloud Armor** (WAF). Mobile‑first responsive UI.
- **APIs**: Cloud Run services; REST + small GraphQL gateway for search.
- **Data stores (prototype vs prod)**  
  - **Prototype (Phase 0)**: **Firestore (Native)** for requests/users/workflows; **Cloud Storage** for files; synthetic data generators.  
  - **Production**: **Cloud SQL for PostgreSQL** for transactional data; **Cloud Storage** for files & renditions.
- **Vector search**: **Vertex AI Matching Engine** (primary scalable ANN index).
- **AI**: **Vertex AI (Gemini 1.5)** for request understanding, summarization, response drafting; **Text Embeddings** for retrieval; RAG over agency indexes.
- **Docs processing**: **Document AI** for OCR/structuring; **DLP API** for PII detection/redaction suggestions (with 100% human review).
- **Workflow**: **Pub/Sub**, **Cloud Workflows**, **Cloud Tasks** for orchestration & scheduled sends.
- **Auth**: **Google Identity Platform** (OIDC/SAML) with **IAP** for the internal console; Google‑first SSO.
- **Email/Delivery**: **Gmail API (Workspace)** for outbound email plus **Cloud Storage Signed URLs** (expiring) for secure portal downloads.
- **Security**: **us-west** residency only; CMEK via **Cloud KMS**; Secret Manager; VPC‑SC perimeter (future).
- **Observability**: Cloud Logging, Monitoring, Traces, Error Reporting, OpenTelemetry; logging sink to **BigQuery** for dashboards.

### Frontend base technical requirements
- **MUI (v5+)** with design tokens; light/dark/high‑contrast; RTL-ready.  
- **TypeScript** strict; ESLint + Prettier via CI.  
- **Data/state**: React Query; URL‑first filters for shareable views.  
- **Forms**: React Hook Form + Zod; MUI fields.  
- **Tables**: MUI Data Grid (virtualized, column filters).  
- **A11y**: WCAG 2.1 AA; keyboard‑first workflows.  
- **PDF/images**: PDF.js viewer; canvas overlays for redaction (UI).  
- **Testing**: Jest + RTL; Playwright for critical flows.  
- **Perf**: Next.js image opt; code‑splitting; CDN; Core Web Vitals budgets.

---

## 3) Components
1. **Public Requester Portal** (public) — request intake, attachment upload, status lookup (phase 2), A11y AA.
2. **Agency Console** (internal) — queues, SLA views, AI match review, redaction UI, approvals, delivery scheduling, audit trail.
3. **Ingestion & Indexing**  
   - **Prototype (simulate sources):** load synthetic corpora & public sample datasets into Cloud Storage; mock connectors emit realistic metadata.  
   - **Future sources:** SharePoint/Exchange, Google Drive/Gmail, file shares/SFTP, case‑management DBs, email, on‑prem DBs, Azure, AWS.  
   - Pipeline: fetch/ingest → normalize → OCR → extract → classify → embed → store → index (Matching Engine).
4. **Search & Match Service** — Hybrid keyword + semantic (Matching Engine). Filters: agency, date range, source, type, sensitivity. Confidence scoring + explainability.
5. **Redaction Service** — PDF viewer/editor (PDF.js) with suggestion overlays. DLP‑based auto‑detect for PII. Versioned outputs; 100% human review required.
6. **Approvals & Legal Review** — routed tasks, comments, diffs, sign‑off, rework loops.
7. **Delivery Service** — combine records into a single PDF (cover sheet + index); schedule via Cloud Tasks; deliver via Gmail API and/or portal links (Signed URLs).
8. **Admin & RBAC** — roles: Admin, Records Officer, Legal Reviewer, Agency Staff, Requester. Multi‑tenant isolation by `agency_id` (row‑level security in SQL).

---

## 4) Data Model (initial)
**Entities**
- `agency(id, name, domain, retention_policy_id, ...)`
- `user(id, agency_id, role, email, name, idp_subject, mfa_enrolled, ...)`
- `request(id, agency_id, requester_name, requester_email, description, received_at, due_at, status, sla_tier, category, ...)`
- `request_message(id, request_id, sender_type, body, created_at, attachments[])`
- `record(id, agency_id, source_system, source_id, title, mime_type, created_at, sensitivity, checksum, storage_uri, ...)`
- `record_text(id, record_id, ocr_text, docai_json, embeddings_vector, lang, ...)`
- `match(id, request_id, record_id, score, rationale, reviewer_id, decision, decided_at, ...)`
- `redaction(id, record_id, version, author_id, doc_uri, redacted_uri, dlp_findings_json, status, ...)`
- `approval(id, subject_type, subject_id, approver_id, status, comments, decided_at, ...)`
- `delivery(id, request_id, package_uri, method, scheduled_for, sent_at, status, ...)`
- `audit(id, actor, action, subject_type, subject_id, ts, ip, context_json)`

**Indexes**: btree on `agency_id`, `status`, timestamps; ANN index in Matching Engine; or `pgvector` in SQL for smaller sets.

---

## 5) AI Design
- **Retrieval**: per‑agency corpora with Vertex **Text Embeddings**; vectors in **Matching Engine**; metadata in Firestore/Cloud SQL.
- **Ranking**: hybrid BM25 + ANN semantic scoring; Top‑N + explanation (nearest‑neighbor distance + matched phrases).
- **Summarization**: Gemini compares candidate records to request; produces justification text.
- **PII Redaction**: DLP detects SSN, DL#, DOB, addresses, minors, health info, and other protected info; **100% human review** required before release.
- **Response drafting**: Gemini with legal templates (placeholders: request_id, date ranges, exemptions, fees); human approval required.
- **Safety**: no model training on agency data; prompts/outputs logged for audit.

---

## 6) Workflows
**A. Request Intake**  
1) Public submits request → `request` created; SLA clock starts.  
2) Classifier tags category, suggests due date; notification to queue.

**B. Record Matching**  
1) Search service retrieves Top‑N by semantic+keyword.  
2) Reviewer confirms/rejects; decisions logged in `match`.

**C. Redaction**  
1) Selected records → OCR (Document AI) → DLP findings.  
2) Editor shows overlays; officer applies/edits masks; versioned redaction saved; **human review mandatory**.

**D. Approvals**  
1) Legal Review; comments, change requests.  
2) Approval recorded; package locked.

**E. Delivery**  
1) Combine as single PDF (cover sheet + index).  
2) Schedule send (Cloud Tasks); **Gmail API** and/or **Signed URL** link; status updated; receipts/audit entries emitted.

---

## 7) APIs (selected)
```
POST   /requests                  # public intake
GET    /requests/:id              # request details + timeline
GET    /requests?status=...       # queues & filters
POST   /requests/:id/matches      # run AI match, return candidates
POST   /records/:id/redactions    # create redaction draft from DLP findings
POST   /packages                  # build combined PDF
POST   /deliveries                # schedule a delivery
POST   /approvals                 # submit/decide an approval task
GET    /audit?subject_type=...&subject_id=...
```
Auth via IAP/Identity Platform JWT; agency context in claims; RBAC middleware for fine‑grained authz.

---

## 8) Security & Compliance
- **AuthN/AuthZ**: Google Identity Platform (SAML/OIDC) + IAP; role‑based permissions; least‑privileged IAM.  
- **Data protection**: CMEK for Storage/SQL; VPC‑SC perimeters; Private Service Connect; TLS 1.2+.  
- **PII**: DLP classification policies; masked logs; **100% human redaction review** gate.  
- **Audit**: Cloud Audit Logs + application audit table; immutable exports to **BigQuery** for eDiscovery.  
- **Compliance**: map to state public‑records statutes; SOC 2 control mappings; FedRAMP Moderate if required.  
- **Residency**: **us‑west** only for Storage/DB/AI endpoints (where supported).  
- **Backups/DR**: automated SQL/Firestore backups; regional replicas within us‑west; RPO ≤ 15m, RTO ≤ 1h.

---

## 9) Scalability & Performance
- Stateless services on Cloud Run with autoscaling; min instances for warm starts.  
- Pub/Sub buffering for ingest/processing spikes.  
- **Traffic assumptions**: ~**100 requests/month**, ~**3 records/request**, avg file **< 10 MB**.  
- **SLOs (initial)**: Search P95 ≤ 2s (≤ 50k docs) and ≤ 4s at larger scale with Matching Engine; redaction viewer P95 ≤ 3s for ≤ 50MB PDFs; delivery schedule adherence ≥ 99.5%.  
- Load testing with k6; traces verified via Cloud Trace.

---

## 10) Observability
- Cloud Logging, Monitoring, Traces, Error Reporting, OpenTelemetry.  
- Logging sink to **BigQuery** for analytics dashboards.  
- Metrics: request turnaround, match precision/recall, redaction edits per doc, approval cycle time, SLA breaches.  
- Alerts: queue backlog, delivery failures, ingestion errors, latency SLO burn.

---

## 11) Deployment & Environments
- **Envs**: local (Docker + emulators) → dev → test → staging → prod.  
- **Emulators (Phase 0)**: Firestore emulator, Pub/Sub emulator; mock wrappers for Vertex/DocAI/DLP for offline demo.  
- **CI/CD**: Cloud Build + GitHub Actions; container image scanning; Terraform for infra.  
- **Migrations**: n/a in Phase 0 (Firestore/Simulation); plan Flyway when moving to Cloud SQL.  
- **Secrets**: Secret Manager + least‑privileged service accounts.  
- **Audit export**: Logging sink to **BigQuery** dataset (e.g., `pr_ai_audit_us_west`).

---

## 12) Phased Delivery
- **Phase 0 (Prototype/Simulation)**: Google‑only stack; Firestore + Cloud Storage; simulated connectors; **Matching Engine**; **mock sends**; full interactive UI with no external source integrations.  
- **Phase 1 (MVP)**: Intake, queues, hybrid search, DLP/DocAI suggestions, approvals, combined PDF, scheduler; single‑agency; Gmail + portal delivery.  
- **Phase 2**: Multi‑agency tenancy; first real connectors (Google Drive/Gmail, SharePoint/Exchange); analytics; requester status portal.  
- **Phase 3**: Additional connectors (file shares/SFTP, case mgmt DBs, on‑prem DBs, Azure, AWS); federated search; translations; eDiscovery.

---

## 13) Decisions from Open Questions
- **Requester portal sends (prototype):** **Mock sends only**; no real emails in Phase 0.  
- **Dummy data scope:** **Multiple agencies (5)** — Public Safety, Transportation, Health, Corrections, Education.  
- **Corpora realism:** Pre‑load additional **public‑domain government PDFs** (non‑PII) to boost realism alongside synthetic docs.  
- **Audit logs export:** Allow export to **BigQuery** for dashboarding; retain app‑level audit table and Cloud Logging.

---

## 14) Appendix — Synthetic Dataset v2
- Includes: `agencies.csv`, `requests.csv`, `records.csv`, `matches.csv`, `redactions.csv`, `approvals.csv`, `deliveries.csv`, plus 14 synthetic PDFs.  
- Agencies: Public Safety, Transportation, Health, Corrections, Education.  
- Prototype delivery methods: `mock_email`, `portal_link`.  
- Note: Any added public‑domain PDFs must be PII‑free and appropriately licensed.

---
